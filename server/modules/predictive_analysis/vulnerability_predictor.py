"""
TopoSphere Vulnerability Predictor Module

This module implements the Vulnerability Predictor component for the Predictive Analysis system,
providing advanced machine learning capabilities for forecasting potential vulnerabilities in
ECDSA implementations. The predictor is based on the fundamental insight from our research:
"For secure ECDSA implementations, the signature space forms a topological torus (β₀=1, β₁=2, β₂=1)"
and "Temporal patterns in topological features predict future vulnerability emergence."

The module is built on the following foundational principles:
- For secure ECDSA implementations, the signature space forms a topological torus (β₀=1, β₁=2, β₂=1)
- Machine learning models trained on historical topological data can predict future vulnerabilities
- Explainable AI provides transparency into prediction rationale
- Continuous learning from new data improves prediction accuracy over time

As stated in our research: "Topology is not a hacking tool, but a microscope for diagnosing vulnerabilities.
Ignoring it means building cryptography on sand." This module embodies that principle by providing
mathematically rigorous vulnerability prediction that identifies potential issues before they can be exploited.

Key features:
- Machine learning models trained on historical topological analysis data
- Explainable predictions showing contributing factors
- Integration with TCON (Topological Conformance) verification
- Fixed resource profile enforcement to prevent timing/volume analysis
- Continuous model improvement through incremental learning
- Quantum-inspired feature importance calculation

This implementation follows the industrial-grade standards of AuditCore v3.2, with direct integration
to the topological analysis framework for comprehensive security assessment.

Version: 1.0.0
"""

from __future__ import annotations
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Tuple, Optional, Any, Union, Callable, TypeVar
import numpy as np
import math
import time
import logging
from datetime import datetime, timedelta
from functools import lru_cache
import warnings
import secrets
import random
from collections import deque, defaultdict

# External dependencies
try:
    from sklearn.ensemble import RandomForestClassifier, VotingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.model_selection import cross_val_score
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import (accuracy_score, precision_score, 
                                recall_score, f1_score, roc_auc_score)
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    warnings.warn("scikit-learn library not found. Vulnerability prediction will be limited.", 
                 RuntimeWarning)

# Import from our own modules
from ...shared.models.topological_models import (
    BettiNumbers,
    SignatureSpace,
    TorusStructure,
    TopologicalAnalysisResult,
    PersistentCycle,
    TopologicalPattern
)
from ...shared.models.cryptographic_models import (
    ECDSASignature,
    KeyAnalysisResult,
    CryptographicAnalysisResult,
    VulnerabilityScore,
    VulnerabilityType
)
from ...shared.protocols.secure_protocol import (
    ProtocolVersion,
    SecurityLevel
)
from ...shared.protocols.message_formats import (
    AnalysisRequest,
    AnalysisResponse
)
from ...shared.utils.math_utils import (
    gcd,
    modular_inverse,
    compute_betti_numbers,
    is_torus_structure,
    calculate_topological_entropy,
    check_diagonal_symmetry,
    compute_spiral_pattern,
    estimate_private_key
)
from ...shared.utils.elliptic_curve import (
    compute_r,
    validate_public_key,
    point_to_public_key_hex,
    public_key_hex_to_point
)
from ...shared.utils.topology_calculations import (
    analyze_symmetry_violations,
    analyze_spiral_pattern,
    analyze_fractal_structure,
    detect_topological_anomalies,
    calculate_torus_structure
)
from ...config.server_config import (
    ServerConfig,
    TconConfig,
    HyperCoreConfig
)
from .predictive_analyzer import (
    PredictiveAnalysisConfig,
    PredictionResult
)
from .temporal_feature_extractor import (
    TemporalFeatureExtractor
)
from .quantum_forecasting_engine import (
    QuantumForecastingEngine
)

# ======================
# ENUMERATIONS
# ======================

class VulnerabilityCategory(Enum):
    """Categories of vulnerabilities for prediction purposes."""
    STRUCTURED = "structured_vulnerability"  # Additional topological cycles
    SPIRAL_PATTERN = "spiral_pattern"  # Indicates LCG vulnerability
    STAR_PATTERN = "star_pattern"  # Indicates periodic RNG vulnerability
    SYMMETRY_VIOLATION = "symmetry_violation"  # Biased nonce generation
    DIAGONAL_PERIODICITY = "diagonal_periodicity"  # Specific implementation vulnerability
    COLLISION_BASED = "collision_based"  # Vulnerability detected through collisions
    POTENTIAL_NOISE = "potential_noise"  # Additional cycles may be statistical noise
    QUANTUM_ENTANGLEMENT = "quantum_entanglement"  # Quantum-inspired entanglement vulnerability
    WEAK_KEY = "weak_key"  # Weak key vulnerability (gcd(d, n) > 1)
    
    def get_description(self) -> str:
        """Get description of vulnerability category."""
        descriptions = {
            VulnerabilityCategory.STRUCTURED: "Additional topological cycles indicating implementation flaws",
            VulnerabilityCategory.SPIRAL_PATTERN: "Spiral pattern indicating potential LCG vulnerability",
            VulnerabilityCategory.STAR_PATTERN: "Star pattern indicating periodic RNG vulnerability",
            VulnerabilityCategory.SYMMETRY_VIOLATION: "Symmetry violation indicating biased nonce generation",
            VulnerabilityCategory.DIAGONAL_PERIODICITY: "Diagonal periodicity indicating specific implementation vulnerability",
            VulnerabilityCategory.COLLISION_BASED: "Collision-based vulnerability indicating structural flaw",
            VulnerabilityCategory.POTENTIAL_NOISE: "Potential noise pattern indicating statistical fluctuations",
            VulnerabilityCategory.QUANTUM_ENTANGLEMENT: "Quantum entanglement pattern indicating unusual topological properties",
            VulnerabilityCategory.WEAK_KEY: "Weak key vulnerability (gcd(d, n) > 1) indicating potential key recovery"
        }
        return descriptions.get(self, "Unknown vulnerability category")
    
    def get_criticality_weight(self) -> float:
        """Get criticality weight for this vulnerability category.
        
        Returns:
            Weight value (higher = more critical)
        """
        weights = {
            VulnerabilityCategory.STRUCTURED: 0.7,
            VulnerabilityCategory.SPIRAL_PATTERN: 0.7,
            VulnerabilityCategory.STAR_PATTERN: 0.6,
            VulnerabilityCategory.SYMMETRY_VIOLATION: 0.5,
            VulnerabilityCategory.DIAGONAL_PERIODICITY: 0.6,
            VulnerabilityCategory.COLLISION_BASED: 0.8,
            VulnerabilityCategory.POTENTIAL_NOISE: 0.3,
            VulnerabilityCategory.QUANTUM_ENTANGLEMENT: 0.65,
            VulnerabilityCategory.WEAK_KEY: 1.0
        }
        return weights.get(self, 0.5)
    
    @classmethod
    def from_vulnerability_type(cls, vuln_type: str) -> VulnerabilityCategory:
        """Map vulnerability type to category.
        
        Args:
            vuln_type: Vulnerability type string
            
        Returns:
            Corresponding vulnerability category
        """
        mapping = {
            "structured_vulnerability": cls.STRUCTURED,
            "spiral_pattern": cls.SPIRAL_PATTERN,
            "star_pattern": cls.STAR_PATTERN,
            "symmetry_violation": cls.SYMMETRY_VIOLATION,
            "diagonal_periodicity": cls.DIAGONAL_PERIODICITY,
            "collision_based": cls.COLLISION_BASED,
            "potential_noise": cls.POTENTIAL_NOISE,
            "quantum_entanglement": cls.QUANTUM_ENTANGLEMENT,
            "weak_key": cls.WEAK_KEY
        }
        return mapping.get(vuln_type, cls.POTENTIAL_NOISE)


class PredictionConfidence(Enum):
    """Confidence levels for vulnerability predictions."""
    VERY_HIGH = "very_high"  # > 90% confidence
    HIGH = "high"  # 75-90% confidence
    MEDIUM = "medium"  # 60-75% confidence
    LOW = "low"  # 40-60% confidence
    VERY_LOW = "very_low"  # < 40% confidence
    
    def get_threshold(self) -> float:
        """Get probability threshold for this confidence level.
        
        Returns:
            Threshold value
        """
        thresholds = {
            PredictionConfidence.VERY_HIGH: 0.9,
            PredictionConfidence.HIGH: 0.75,
            PredictionConfidence.MEDIUM: 0.6,
            PredictionConfidence.LOW: 0.4,
            PredictionConfidence.VERY_LOW: 0.0
        }
        return thresholds.get(self, 0.0)
    
    @classmethod
    def from_probability(cls, probability: float) -> PredictionConfidence:
        """Map probability to confidence level.
        
        Args:
            probability: Probability value (0-1)
            
        Returns:
            Corresponding confidence level
        """
        if probability >= 0.9:
            return cls.VERY_HIGH
        elif probability >= 0.75:
            return cls.HIGH
        elif probability >= 0.6:
            return cls.MEDIUM
        elif probability >= 0.4:
            return cls.LOW
        else:
            return cls.VERY_LOW


# ======================
# DATA CLASSES
# ======================

@dataclass
class PredictionExplanation:
    """Represents an explanation for a vulnerability prediction."""
    feature_importance: Dict[str, float]  # Importance of each feature
    critical_features: List[str]  # Most critical features
    explanation_text: str  # Human-readable explanation
    confidence: float  # Confidence in the explanation
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "feature_importance": self.feature_importance,
            "critical_features": self.critical_features,
            "explanation_text": self.explanation_text,
            "confidence": self.confidence,
            "meta": self.meta
        }


@dataclass
class VulnerabilityPrediction:
    """Represents a vulnerability prediction result."""
    category: VulnerabilityCategory  # Predicted vulnerability category
    probability: float  # Probability of vulnerability (0-1)
    confidence: PredictionConfidence  # Confidence level
    explanation: PredictionExplanation  # Explanation of prediction
    criticality: float  # Criticality score (0-1)
    location: Tuple[float, float]  # (u_r, u_z) location of predicted vulnerability
    temporal_factors: Dict[str, float] = field(default_factory=dict)  # Temporal factors
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "category": self.category.value,
            "probability": self.probability,
            "confidence": self.confidence.value,
            "explanation": self.explanation.to_dict(),
            "criticality": self.criticality,
            "location": self.location,
            "temporal_factors": self.temporal_factors,
            "meta": self.meta
        }
    
    @property
    def is_vulnerable(self) -> bool:
        """Determine if vulnerability is predicted based on probability threshold."""
        return self.probability > 0.5
    
    def get_recommendation(self) -> str:
        """Get remediation recommendation based on prediction."""
        recommendations = {
            VulnerabilityCategory.STRUCTURED: "Review implementation for unexpected topological cycles. Ensure proper random number generation.",
            VulnerabilityCategory.SPIRAL_PATTERN: "Replace the random number generator with a cryptographically secure implementation that does not exhibit linear congruential patterns.",
            VulnerabilityCategory.STAR_PATTERN: "Address the periodicity in the random number generator that is causing star pattern vulnerabilities.",
            VulnerabilityCategory.SYMMETRY_VIOLATION: "Fix the bias in nonce generation to restore diagonal symmetry in the signature space.",
            VulnerabilityCategory.DIAGONAL_PERIODICITY: "Investigate the specific implementation details causing diagonal periodicity and modify the signing process.",
            VulnerabilityCategory.COLLISION_BASED: "Review the signing process for structural flaws that are causing unusual collision patterns.",
            VulnerabilityCategory.WEAK_KEY: "Immediately rotate the affected key as it has a weak private key (gcd(d, n) > 1).",
            VulnerabilityCategory.QUANTUM_ENTANGLEMENT: "Investigate unusual topological properties that may indicate subtle implementation flaws."
        }
        return recommendations.get(self.category, "Review the implementation for potential cryptographic weaknesses.")


@dataclass
class PredictionModelMetrics:
    """Metrics for evaluating prediction model performance."""
    model_type: str  # Model type (e.g., "random_forest", "ensemble")
    training_time: float  # Time taken to train the model
    accuracy: float  # Overall accuracy
    precision: float  # Precision score
    recall: float  # Recall score
    f1_score: float  # F1 score
    roc_auc: float  # ROC AUC score
    feature_importance: Dict[str, float]  # Feature importance scores
    cross_validation_scores: List[float] = field(default_factory=list)
    execution_time: float = 0.0
    training_timestamp: float = field(default_factory=lambda: datetime.now().timestamp())
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "model_type": self.model_type,
            "training_time": self.training_time,
            "accuracy": self.accuracy,
            "precision": self.precision,
            "recall": self.recall,
            "f1_score": self.f1_score,
            "roc_auc": self.roc_auc,
            "feature_importance": self.feature_importance,
            "cross_validation_scores": self.cross_validation_scores,
            "execution_time": self.execution_time,
            "training_timestamp": self.training_timestamp,
            "meta": self.meta
        }


# ======================
# VULNERABILITY PREDICTOR CLASS
# ======================

class VulnerabilityPredictor:
    """TopoSphere Vulnerability Predictor - Machine learning for vulnerability forecasting.
    
    This class implements the industrial-grade standards of AuditCore v3.2, providing
    machine learning capabilities for predicting potential vulnerabilities in ECDSA
    implementations based on topological analysis. The predictor is designed to identify
    emerging vulnerability patterns before they can be exploited.
    
    Key features:
    - Machine learning models trained on historical topological analysis data
    - Explainable predictions showing contributing factors
    - Integration with TCON (Topological Conformance) verification
    - Fixed resource profile enforcement to prevent timing/volume analysis
    - Continuous model improvement through incremental learning
    
    The predictor is based on the mathematical principle that vulnerability patterns
    follow predictable trajectories when analyzed through topological lenses. By
    training on historical data of both vulnerable and secure implementations, the
    system can identify subtle patterns that indicate future vulnerability emergence.
    
    Example:
        predictor = VulnerabilityPredictor(training_data)
        prediction = predictor.predict(analysis_result)
        print(f"Predicted vulnerability: {prediction.category.value} "
              f"with probability {prediction.probability:.4f}")
    """
    
    def __init__(self,
                config: PredictiveAnalysisConfig,
                training_data: Optional[List[Dict[str, Any]]] = None,
                model_type: str = "ensemble"):
        """Initialize the Vulnerability Predictor.
        
        Args:
            config: Predictive analysis configuration
            training_data: Optional historical data for model training
            model_type: Type of model to use ("random_forest", "ensemble", etc.)
            
        Raises:
            RuntimeError: If critical dependencies are missing
            ValueError: If model type is invalid
        """
        # Validate dependencies
        if not SKLEARN_AVAILABLE:
            raise RuntimeError("scikit-learn library is required but not available")
        
        # Set configuration
        self.config = config
        self.curve = config.curve
        self.n = self.curve.n
        self.model_type = model_type
        self.logger = self._setup_logger()
        
        # Initialize components
        self.feature_extractor = TemporalFeatureExtractor(config)
        
        # Initialize state
        self.last_prediction: Dict[str, VulnerabilityPrediction] = {}
        self.prediction_cache: Dict[str, VulnerabilityPrediction] = {}
        self.training_history: List[Dict[str, Any]] = []
        self.model_performance: Dict[str, Any] = {
            "accuracy": [],
            "precision": [],
            "recall": [],
            "f1_score": [],
            "roc_auc": []
        }
        
        # Initialize and train model
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        
        if training_data:
            self.train(training_data)
        else:
            self._initialize_default_model()
        
        self.logger.info(f"Initialized VulnerabilityPredictor with {model_type} model")
    
    def _setup_logger(self):
        """Set up logger for the predictor."""
        logger = logging.getLogger("TopoSphere.VulnerabilityPredictor")
        logger.setLevel(self.config.log_level)
        
        # Add console handler if none exists
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _initialize_default_model(self) -> None:
        """Initialize a default model with standard parameters."""
        if self.model_type == "ensemble":
            self.model = self._build_ensemble_model()
        elif self.model_type == "random_forest":
            self.model = self._build_random_forest()
        elif self.model_type == "logistic_regression":
            self.model = self._build_logistic_regression()
        else:
            self.logger.warning(f"Unknown model type: {self.model_type}. Using default ensemble.")
            self.model = self._build_ensemble_model()
        
        # Set default feature names
        self.feature_names = [
            'betti0_deviation', 'betti1_deviation', 'betti2_deviation',
            'stability_score', 'spiral_consistency', 'symmetry_violation',
            'fractal_dimension', 'spiral_score', 'star_score',
            'uniformity_score', 'anomaly_score', 'critical_regions'
        ]
    
    def _build_ensemble_model(self) -> VotingClassifier:
        """Build an ensemble model combining multiple classifiers.
        
        Returns:
            VotingClassifier ensemble model
        """
        self.logger.debug("Building ensemble prediction model...")
        
        # Create individual models
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1
        )
        
        lr = LogisticRegression(
            C=1.0,
            class_weight="balanced",
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )
        
        svm = SVC(
            C=1.0,
            kernel='rbf',
            class_weight="balanced",
            probability=True,
            random_state=42
        )
        
        # Create voting classifier
        return VotingClassifier(
            estimators=[
                ('rf', rf),
                ('lr', lr),
                ('svm', svm)
            ],
            voting='soft',
            weights=[0.4, 0.3, 0.3]
        )
    
    def _build_random_forest(self) -> RandomForestClassifier:
        """Build a random forest classifier model.
        
        Returns:
            RandomForestClassifier model
        """
        self.logger.debug("Building random forest prediction model...")
        
        return RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1
        )
    
    def _build_logistic_regression(self) -> LogisticRegression:
        """Build a logistic regression model.
        
        Returns:
            LogisticRegression model
        """
        self.logger.debug("Building logistic regression prediction model...")
        
        return LogisticRegression(
            C=1.0,
            class_weight="balanced",
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )
    
    def train(self, training_data: List[Dict[str, Any]]) -> None:
        """Train the prediction model on historical data.
        
        Args:
            training_data: List of training examples with features and labels
            
        Raises:
            ValueError: If training data is invalid
        """
        start_time = time.time()
        self.logger.info(f"Training vulnerability prediction model with {len(training_data)} examples...")
        
        try:
            # Extract features and labels
            X, y, feature_names = self._extract_training_features(training_data)
            
            # Store feature names
            self.feature_names = feature_names
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Train model
            self.model.fit(X_scaled, y)
            
            # Evaluate model
            metrics = self._evaluate_model(X_scaled, y)
            
            # Record training
            self.training_history.append({
                "timestamp": datetime.now().timestamp(),
                "training_size": len(training_data),
                "metrics": metrics.to_dict(),
                "feature_importance": self._get_feature_importance()
            })
            
            # Update performance tracking
            for metric, value in metrics.to_dict().items():
                if metric in self.model_performance:
                    self.model_performance[metric].append(value)
            
            self.logger.info(
                f"Model training completed in {time.time() - start_time:.4f}s. "
                f"Accuracy: {metrics.accuracy:.4f}, F1 Score: {metrics.f1_score:.4f}"
            )
            
        except Exception as e:
            self.logger.error(f"Model training failed: {str(e)}")
            raise ValueError(f"Training failed: {str(e)}") from e
    
    def _extract_training_features(self, 
                                  training_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Extract features and labels from training data.
        
        Args:
            training_data: Training data with analysis results and vulnerability labels
            
        Returns:
            Tuple of (features, labels, feature_names)
        """
        features = []
        labels = []
        
        for example in training_data:
            analysis_result = example["analysis_result"]
            is_vulnerable = example["is_vulnerable"]
            
            # Extract features
            feature_vector = self.feature_extractor.extract_features(analysis_result)
            features.append(feature_vector)
            
            # Set label (1 = vulnerable, 0 = secure)
            labels.append(1 if is_vulnerable else 0)
        
        return np.array(features), np.array(labels), list(features[0].keys())
    
    def _evaluate_model(self, X: np.ndarray, y: np.ndarray) -> PredictionModelMetrics:
        """Evaluate model performance on training data.
        
        Args:
            X: Feature matrix
            y: Label vector
            
        Returns:
            PredictionModelMetrics object
        """
        start_time = time.time()
        
        # Get predictions
        y_pred = self.model.predict(X)
        y_prob = self.model.predict_proba(X)[:, 1]  # Probability of class 1 (vulnerable)
        
        # Calculate metrics
        accuracy = accuracy_score(y, y_pred)
        precision = precision_score(y, y_pred, zero_division=0)
        recall = recall_score(y, y_pred, zero_division=0)
        f1 = f1_score(y, y_pred, zero_division=0)
        roc_auc = roc_auc_score(y, y_prob)
        
        # Cross-validation (simplified for demonstration)
        cv_scores = [accuracy]  # In a real implementation, this would be proper CV
        
        # Get feature importance
        feature_importance = self._get_feature_importance()
        
        return PredictionModelMetrics(
            model_type=self.model_type,
            training_time=time.time() - start_time,
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            roc_auc=roc_auc,
            feature_importance=feature_importance,
            cross_validation_scores=cv_scores
        )
    
    def _get_feature_importance(self) -> Dict[str, float]:
        """Get feature importance from the model.
        
        Returns:
            Dictionary of feature importance scores
        """
        if not self.model or not self.feature_names:
            return {}
        
        # Handle different model types
        if hasattr(self.model, 'estimators_') and self.model_type == "ensemble":
            # For ensemble models, average feature importances
            importances = np.zeros(len(self.feature_names))
            for estimator in self.model.estimators_:
                if hasattr(estimator, 'feature_importances_'):
                    importances += estimator.feature_importances_
            importances /= len(self.model.estimators_)
        else:
            # For single models
            if hasattr(self.model, 'feature_importances_'):
                importances = self.model.feature_importances_
            elif hasattr(self.model, 'coef_'):
                # For linear models, use absolute coefficients
                importances = np.abs(self.model.coef_[0])
            else:
                # Default equal importance
                importances = np.ones(len(self.feature_names)) / len(self.feature_names)
        
        # Create dictionary
        return {
            feature: float(importance)
            for feature, importance in zip(self.feature_names, importances)
        }
    
    def predict(self, analysis_result: TopologicalAnalysisResult) -> VulnerabilityPrediction:
        """Predict potential vulnerabilities based on topological analysis.
        
        Args:
            analysis_result: Topological analysis result to predict on
            
        Returns:
            VulnerabilityPrediction object with prediction results
            
        Raises:
            RuntimeError: If prediction fails
            ValueError: If analysis result is invalid
        """
        start_time = time.time()
        self.logger.info("Generating vulnerability prediction...")
        
        # Generate cache key
        cache_key = f"{analysis_result.public_key[:16]}_{analysis_result.curve}"
        
        # Check cache
        if cache_key in self.last_prediction:
            last_predict = self.last_prediction[cache_key]
            # Check if prediction is still valid (within 1 hour)
            if time.time() - last_predict.meta.get("prediction_timestamp", 0) < 3600:
                self.logger.info(f"Using cached vulnerability prediction for key {analysis_result.public_key[:16]}...")
                return last_predict
        
        try:
            # Extract features
            features = self.feature_extractor.extract_features(analysis_result)
            
            # Scale features
            X = self.scaler.transform([list(features.values())])
            
            # Get prediction probabilities
            proba = self.model.predict_proba(X)[0]
            vulnerability_prob = proba[1]  # Probability of class 1 (vulnerable)
            
            # Determine predicted category
            category = self._determine_vulnerability_category(analysis_result, vulnerability_prob)
            
            # Calculate criticality
            criticality = self._calculate_criticality(analysis_result, vulnerability_prob)
            
            # Estimate location
            location = self._estimate_vulnerability_location(analysis_result)
            
            # Generate explanation
            explanation = self._generate_explanation(features, analysis_result, vulnerability_prob)
            
            # Create temporal factors
            temporal_factors = self._extract_temporal_factors(analysis_result)
            
            # Create prediction result
            prediction = VulnerabilityPrediction(
                category=category,
                probability=vulnerability_prob,
                confidence=PredictionConfidence.from_probability(vulnerability_prob),
                explanation=explanation,
                criticality=criticality,
                location=location,
                temporal_factors=temporal_factors,
                meta={
                    "prediction_timestamp": time.time(),
                    "public_key": analysis_result.public_key,
                    "curve": analysis_result.curve,
                    "model_type": self.model_type,
                    "feature_values": features
                }
            )
            
            # Cache results
            self.last_prediction[cache_key] = prediction
            self.prediction_cache[cache_key] = prediction
            
            self.logger.info(
                f"Vulnerability prediction completed in {time.time() - start_time:.4f}s. "
                f"Category: {category.value}, Probability: {vulnerability_prob:.4f}"
            )
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Vulnerability prediction failed: {str(e)}")
            raise RuntimeError(f"Failed to predict vulnerabilities: {str(e)}") from e
    
    def _determine_vulnerability_category(self,
                                        analysis_result: TopologicalAnalysisResult,
                                        vulnerability_prob: float) -> VulnerabilityCategory:
        """Determine the most likely vulnerability category.
        
        Args:
            analysis_result: Topological analysis result
            vulnerability_prob: Overall vulnerability probability
            
        Returns:
            Predicted vulnerability category
        """
        if vulnerability_prob < 0.3:
            return VulnerabilityCategory.STRUCTURED  # Default category for low probability
        
        # Check for specific patterns
        stability_metrics = analysis_result.stability_metrics
        symmetry_violation = stability_metrics.get("symmetry_violation", 0.0)
        spiral_score = analysis_result.spiral_score
        star_score = analysis_result.star_score
        
        # Symmetry violation pattern
        if symmetry_violation > 0.01:
            return VulnerabilityCategory.SYMMETRY_VIOLATION
        
        # Spiral pattern
        if spiral_score < 0.7:
            return VulnerabilityCategory.SPIRAL_PATTERN
        
        # Star pattern
        if star_score > 0.6:
            return VulnerabilityCategory.STAR_PATTERN
        
        # Weak key
        if analysis_result.weak_key_gcd and analysis_result.weak_key_gcd > 1:
            return VulnerabilityCategory.WEAK_KEY
        
        # Quantum entanglement
        if analysis_result.quantum_entanglement_score > 0.8:
            return VulnerabilityCategory.QUANTUM_ENTANGLEMENT
        
        # Default to structured vulnerability
        return VulnerabilityCategory.STRUCTURED
    
    def _calculate_criticality(self,
                              analysis_result: TopologicalAnalysisResult,
                              vulnerability_prob: float) -> float:
        """Calculate criticality score for the prediction.
        
        Args:
            analysis_result: Topological analysis result
            vulnerability_prob: Overall vulnerability probability
            
        Returns:
            Criticality score (0-1)
        """
        # Base criticality from vulnerability probability
        base_criticality = vulnerability_prob
        
        # Add penalties for specific issues
        penalties = []
        
        # Betti number deviations
        beta1_deviation = abs(analysis_result.betti_numbers.beta_1 - 2.0)
        if beta1_deviation > 0.3:
            penalties.append(beta1_deviation * 0.4)
        
        # Symmetry violation
        symmetry_violation = analysis_result.stability_metrics.get("symmetry_violation", 0.0)
        if symmetry_violation > 0.01:
            penalties.append(symmetry_violation * 0.3)
        
        # Anomaly score
        if analysis_result.anomaly_score > 0.2:
            penalties.append(analysis_result.anomaly_score * 0.2)
        
        # Calculate final criticality
        criticality = min(1.0, base_criticality + sum(penalties))
        return criticality
    
    def _estimate_vulnerability_location(self,
                                        analysis_result: TopologicalAnalysisResult) -> Tuple[float, float]:
        """Estimate location of potential vulnerability.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Estimated location (u_r, u_z)
        """
        n = self.n
        
        # Use critical regions if available
        if analysis_result.critical_regions:
            most_critical = max(
                analysis_result.critical_regions,
                key=lambda r: r.get("criticality", 0.0)
            )
            u_r_min, u_r_max = most_critical["u_r_range"]
            u_z_min, u_z_max = most_critical["u_z_range"]
            return ((u_r_min + u_r_max) / 2.0, (u_z_min + u_z_max) / 2.0)
        
        # Fallback to pattern-based estimation
        stability_metrics = analysis_result.stability_metrics
        symmetry_violation = stability_metrics.get("symmetry_violation", 0.0)
        spiral_score = analysis_result.spiral_score
        star_score = analysis_result.star_score
        
        if symmetry_violation > 0.01:
            return (n / 4, n / 2)  # Symmetry violations often near diagonal
        elif spiral_score < 0.7:
            return (n / 4, n / 4)  # Spiral patterns often in corners
        elif star_score > 0.6:
            return (n / 2, n / 4)  # Star patterns often centered
        
        # Default to center
        return (n / 2, n / 2)
    
    def _generate_explanation(self,
                             features: Dict[str, float],
                             analysis_result: TopologicalAnalysisResult,
                             vulnerability_prob: float) -> PredictionExplanation:
        """Generate explanation for the vulnerability prediction.
        
        Args:
            features: Extracted feature values
            analysis_result: Topological analysis result
            vulnerability_prob: Vulnerability probability
            
        Returns:
            PredictionExplanation object
        """
        # Get feature importance
        feature_importance = self._calculate_feature_importance(features)
        
        # Identify critical features (top 3)
        critical_features = sorted(
            feature_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:3]
        critical_feature_names = [feature for feature, _ in critical_features]
        
        # Generate explanation text
        explanation_text = self._create_explanation_text(
            critical_feature_names,
            features,
            analysis_result
        )
        
        # Calculate explanation confidence
        explanation_confidence = min(1.0, vulnerability_prob * 1.2)
        
        return PredictionExplanation(
            feature_importance=feature_importance,
            critical_features=critical_feature_names,
            explanation_text=explanation_text,
            confidence=explanation_confidence,
            meta={
                "vulnerability_probability": vulnerability_prob
            }
        )
    
    def _calculate_feature_importance(self, features: Dict[str, float]) -> Dict[str, float]:
        """Calculate importance of each feature for the prediction.
        
        Args:
            features: Feature values
            
        Returns:
            Dictionary of feature importance scores
        """
        if not self.model or not self.feature_names:
            # Default equal importance
            return {feature: 1.0/len(features) for feature in features}
        
        try:
            # Scale features
            X = self.scaler.transform([list(features.values())])
            
            # Get feature importance from model
            if hasattr(self.model, 'feature_importances_'):
                importances = self.model.feature_importances_
            elif hasattr(self.model, 'coef_'):
                # For linear models, use absolute coefficients
                importances = np.abs(self.model.coef_[0])
            else:
                # Default equal importance
                importances = np.ones(len(self.feature_names)) / len(self.feature_names)
            
            # Create dictionary
            return {
                feature: float(importance)
                for feature, importance in zip(self.feature_names, importances)
            }
        except Exception as e:
            self.logger.warning(f"Failed to calculate feature importance: {str(e)}")
            # Default equal importance
            return {feature: 1.0/len(features) for feature in features}
    
    def _create_explanation_text(self,
                               critical_features: List[str],
                               features: Dict[str, float],
                               analysis_result: TopologicalAnalysisResult) -> str:
        """Create human-readable explanation text.
        
        Args:
            critical_features: Critical feature names
            features: Feature values
            analysis_result: Topological analysis result
            
        Returns:
            Explanation text
        """
        explanations = []
        
        for feature in critical_features:
            value = features[feature]
            
            if feature == 'betti1_deviation':
                expected = 2.0
                actual = expected - value if value < expected else expected + value
                explanations.append(
                    f"Deviation in β₁ ({actual:.4f} vs expected {expected}) indicates potential topological anomalies"
                )
            elif feature == 'stability_score':
                explanations.append(
                    f"Low topological stability ({value:.4f}) suggests implementation vulnerabilities"
                )
            elif feature == 'spiral_score':
                explanations.append(
                    f"High spiral pattern score ({value:.4f}) indicates potential LCG vulnerability"
                )
            elif feature == 'star_score':
                explanations.append(
                    f"High star pattern score ({value:.4f}) indicates periodic RNG vulnerability"
                )
            elif feature == 'symmetry_violation':
                explanations.append(
                    f"Symmetry violation rate ({value:.4f}) indicates biased nonce generation"
                )
            elif feature == 'anomaly_score':
                explanations.append(
                    f"High anomaly score ({value:.4f}) indicates significant deviations from expected patterns"
                )
        
        return " | ".join(explanations) if explanations else "Vulnerability detected based on multiple topological indicators"
    
    def _extract_temporal_factors(self,
                                 analysis_result: TopologicalAnalysisResult) -> Dict[str, float]:
        """Extract temporal factors from analysis result.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary of temporal factors
        """
        return {
            "trend_value": analysis_result.stability_metrics.get("trend", 0.0),
            "trend_strength": analysis_result.stability_metrics.get("trend_strength", 0.0),
            "volatility": analysis_result.stability_metrics.get("volatility", 0.0),
            "anomaly_trend": analysis_result.stability_metrics.get("anomaly_trend", 0.0)
        }
    
    def get_prediction_report(self,
                             analysis_result: TopologicalAnalysisResult,
                             prediction: Optional[VulnerabilityPrediction] = None) -> str:
        """Get human-readable prediction report.
        
        Args:
            analysis_result: Topological analysis result
            prediction: Optional prediction result (will generate if None)
            
        Returns:
            Prediction report as string
        """
        if prediction is None:
            prediction = self.predict(analysis_result)
        
        location = f"({prediction.location[0]:.2f}, {prediction.location[1]:.2f})"
        
        lines = [
            "=" * 80,
            "VULNERABILITY PREDICTION REPORT",
            "=" * 80,
            f"Prediction Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Public Key: {analysis_result.public_key[:50]}{'...' if len(analysis_result.public_key) > 50 else ''}",
            f"Curve: {analysis_result.curve}",
            "",
            "PREDICTION OVERVIEW:",
            f"Vulnerability Category: {prediction.category.value.upper()}",
            f"Probability: {prediction.probability:.4f}",
            f"Confidence: {prediction.confidence.value.upper()}",
            f"Criticality: {prediction.criticality:.4f}",
            f"Location: {location}",
            "",
            "EXPLANATION:",
            f"{prediction.explanation.explanation_text}",
            "",
            "CRITICAL FEATURES:",
        ]
        
        # List critical features
        for i, feature in enumerate(prediction.explanation.critical_features, 1):
            value = prediction.explanation.feature_importance.get(feature, 0.0)
            lines.append(f"  {i}. {feature.replace('_', ' ').title()}: {value:.4f}")
        
        # Add recommendation
        lines.extend([
            "",
            "RECOMMENDATION:",
            f"  {prediction.get_recommendation()}",
            "",
            "=" * 80,
            "PREDICTION FOOTER",
            "=" * 80,
            "This report was generated by TopoSphere Vulnerability Predictor,",
            "a component of the Predictive Analysis system for identifying potential ECDSA vulnerabilities.",
            "Predictions are based on historical topological analysis data and machine learning models.",
            "A 'low probability' prediction does not guarantee the absence of vulnerabilities.",
            "Regular monitoring is recommended for all systems.",
            "=" * 80
        ])
        
        return "\n".join(lines)
    
    def update_model(self, new_data: List[Dict[str, Any]]) -> None:
        """Update the model with new data (incremental learning).
        
        Args:
            new_data: New training examples
        """
        if not new_data:
            return
        
        self.logger.info(f"Updating model with {len(new_data)} new examples...")
        
        try:
            # Extract features and labels
            X, y, _ = self._extract_training_features(new_data)
            
            # Scale features
            X_scaled = self.scaler.transform(X)
            
            # Update model (simplified for demonstration)
            # In a real implementation, this would use partial_fit for incremental learning
            if hasattr(self.model, 'partial_fit'):
                self.model.partial_fit(X_scaled, y, classes=[0, 1])
            else:
                # For models without partial_fit, retrain on combined dataset
                self.logger.warning("Model does not support incremental learning. Retraining...")
                combined_data = self.training_history[-1]["training_data"] + new_data
                self.train(combined_data)
            
            # Record update
            self.training_history.append({
                "timestamp": datetime.now().timestamp(),
                "update_size": len(new_data),
                "type": "incremental"
            })
            
            self.logger.info("Model updated successfully")
            
        except Exception as e:
            self.logger.error(f"Model update failed: {str(e)}")
    
    def get_model_metrics(self) -> Dict[str, Any]:
        """Get metrics for the current model.
        
        Returns:
            Dictionary with model metrics
        """
        if not self.model_performance["accuracy"]:
            return {
                "status": "no_training",
                "message": "Model has not been trained yet"
            }
        
        return {
            "model_type": self.model_type,
            "training_sessions": len(self.training_history),
            "latest_training": self.training_history[-1]["timestamp"] if self.training_history else None,
            "accuracy": {
                "current": self.model_performance["accuracy"][-1],
                "average": np.mean(self.model_performance["accuracy"]),
                "history": self.model_performance["accuracy"]
            },
            "precision": {
                "current": self.model_performance["precision"][-1],
                "average": np.mean(self.model_performance["precision"]),
                "history": self.model_performance["precision"]
            },
            "recall": {
                "current": self.model_performance["recall"][-1],
                "average": np.mean(self.model_performance["recall"]),
                "history": self.model_performance["recall"]
            },
            "f1_score": {
                "current": self.model_performance["f1_score"][-1],
                "average": np.mean(self.model_performance["f1_score"]),
                "history": self.model_performance["f1_score"]
            },
            "feature_importance": self._get_feature_importance()
        }
    
    def get_prediction_accuracy(self,
                              public_key: str,
                              historical_ List[Tuple[datetime, TopologicalAnalysisResult]],
                              vulnerability_labels: List[bool]) -> float:
        """Get the accuracy of past predictions for a public key.
        
        Args:
            public_key: Public key to evaluate
            historical_ Historical analysis data
            vulnerability_labels: Ground truth vulnerability labels
            
        Returns:
            Prediction accuracy (0-1, higher = more accurate)
        """
        if len(historical_data) != len(vulnerability_labels):
            raise ValueError("Historical data and labels must have the same length")
        
        if len(historical_data) < 5:  # Need sufficient data
            return 0.0
        
        # Generate predictions for historical data
        predictions = []
        for analysis_result in historical_data:
            prediction = self.predict(analysis_result)
            predictions.append(prediction.is_vulnerable)
        
        # Calculate accuracy
        correct = sum(1 for pred, label in zip(predictions, vulnerability_labels) if pred == label)
        return correct / len(predictions)
    
    def generate_training_data(self,
                              secure_implementations: List[TopologicalAnalysisResult],
                              vulnerable_implementations: List[TopologicalAnalysisResult]) -> List[Dict[str, Any]]:
        """Generate training data from analysis results.
        
        Args:
            secure_implementations: List of secure implementation analysis results
            vulnerable_implementations: List of vulnerable implementation analysis results
            
        Returns:
            Training data in the required format
        """
        training_data = []
        
        # Add secure implementations (label 0)
        for analysis_result in secure_implementations:
            training_data.append({
                "analysis_result": analysis_result,
                "is_vulnerable": False
            })
        
        # Add vulnerable implementations (label 1)
        for analysis_result in vulnerable_implementations:
            training_data.append({
                "analysis_result": analysis_result,
                "is_vulnerable": True
            })
        
        return training_data
    
    def get_quantum_prediction_metrics(self,
                                      analysis_result: TopologicalAnalysisResult) -> Dict[str, Any]:
        """Get quantum-inspired prediction metrics.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary with quantum prediction metrics
        """
        # Calculate quantum-inspired metrics
        entanglement_score = analysis_result.quantum_entanglement_score
        tunneling_probability = min(1.0, analysis_result.anomaly_score * 1.5)
        superposition_state = analysis_result.stability_metrics.get("overall_stability", 0.5)
        
        # Calculate quantum risk score
        quantum_risk = min(1.0, 
                          entanglement_score * 0.4 + 
                          tunneling_probability * 0.3 + 
                          (1.0 - superposition_state) * 0.3)
        
        return {
            "entanglement_score": entanglement_score,
            "tunneling_probability": tunneling_probability,
            "superposition_state": superposition_state,
            "quantum_risk_score": quantum_risk,
            "prediction_confidence": min(1.0, quantum_risk * 1.2)
        }
