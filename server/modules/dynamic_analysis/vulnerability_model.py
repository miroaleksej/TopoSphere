"""
TopoSphere Vulnerability Model Module

This module implements the Vulnerability Model component for the Dynamic Analysis system,
providing mathematically rigorous identification and classification of ECDSA implementation
vulnerabilities. The model is based on the fundamental insight from our research:
"For secure ECDSA implementations, the signature space forms a topological torus (β₀=1, β₁=2, β₂=1)"
and "Deviations from expected topological structure indicate potential vulnerabilities."

The module is built on the following foundational principles:
- For secure ECDSA implementations, the signature space forms a topological torus (β₀=1, β₁=2, β₂=1)
- For any public key Q = dG and for any pair (u_r, u_z) ∈ ℤ_n × ℤ_n, there exists a signature (r, s, z)
- Vulnerability patterns such as spiral, star, and fractal structures indicate potential vulnerabilities
- Persistent homology and cycle analysis provide mathematical rigor for vulnerability identification

As stated in our research: "Topology is not a hacking tool, but a microscope for diagnosing vulnerabilities.
Ignoring it means building cryptography on sand." This module embodies that principle by providing
mathematically rigorous vulnerability identification that detects issues while maintaining privacy
guarantees and resource efficiency.

Key features:
- Comprehensive vulnerability classification system with precise localization
- Quantum-inspired vulnerability scoring with adaptive parameters
- Integration with TCON (Topological Conformance) verification
- Fixed resource profile enforcement to prevent timing/volume analysis
- Differential privacy mechanisms to prevent algorithm recovery
- Multiscale analysis for vulnerability detection across different scales

This implementation follows the industrial-grade standards of AuditCore v3.2, with direct integration
to the topological analysis framework for comprehensive security assessment.

Version: 1.0.0
"""

from __future__ import annotations
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import numpy as np
import math
import time
import logging
from datetime import datetime, timedelta
from functools import lru_cache
import warnings
import secrets

# External dependencies
try:
    from giotto_tda import wasserstein_distance
    TDALIB_AVAILABLE = True
except ImportError:
    TDALIB_AVAILABLE = False
    warnings.warn("giotto-tda library not found. Vulnerability modeling will be limited.", 
                 RuntimeWarning)

# Import from our own modules
from ...shared.models.topological_models import (
    BettiNumbers,
    SignatureSpace,
    TorusStructure,
    TopologicalAnalysisResult,
    PersistentCycle,
    TopologicalPattern
)
from ...shared.models.cryptographic_models import (
    ECDSASignature,
    KeyAnalysisResult,
    CryptographicAnalysisResult,
    VulnerabilityScore,
    VulnerabilityType
)
from ...shared.protocols.secure_protocol import (
    ProtocolVersion,
    SecurityLevel
)
from ...shared.protocols.message_formats import (
    AnalysisRequest,
    AnalysisResponse
)
from ...shared.utils.math_utils import (
    gcd,
    modular_inverse,
    compute_betti_numbers,
    is_torus_structure,
    calculate_topological_entropy,
    check_diagonal_symmetry,
    compute_spiral_pattern,
    estimate_private_key
)
from ...shared.utils.elliptic_curve import (
    compute_r,
    validate_public_key,
    point_to_public_key_hex,
    public_key_hex_to_point
)
from ...shared.utils.topology_calculations import (
    analyze_symmetry_violations,
    analyze_spiral_pattern,
    analyze_fractal_structure,
    detect_topological_anomalies,
    calculate_torus_structure
)
from ...config.server_config import (
    ServerConfig,
    TconConfig,
    HyperCoreConfig
)
from .dynamic_analyzer import (
    DynamicAnalysisConfig,
    DynamicAnalysisResult
)
from .vulnerability_detector import (
    RealTimeVulnerabilityDetector
)

# ======================
# ENUMERATIONS
# ======================

class VulnerabilityPattern(Enum):
    """Types of vulnerability patterns detected in topological analysis."""
    SPIRAL = "spiral_pattern"  # Indicates LCG vulnerability
    STAR = "star_pattern"  # Indicates periodic RNG vulnerability
    SYMMETRY_VIOLATION = "symmetry_violation"  # Biased nonce generation
    DIAGONAL_PERIODICITY = "diagonal_periodicity"  # Specific implementation vulnerability
    COLLISION_BASED = "collision_based"  # Vulnerability detected through collisions
    GRADIENT_BASED = "gradient_based"  # Vulnerability detected through gradient analysis
    STRUCTURED = "structured_vulnerability"  # Topological structure does not match expected torus
    POTENTIAL_NOISE = "potential_noise"  # Additional cycles may be statistical noise
    FRACTAL = "fractal"  # Fractal pattern indicating structured vulnerability
    WEAK_KEY = "weak_key"  # Weak key vulnerability (gcd(d, n) > 1)
    QUANTUM_ENTANGLEMENT = "quantum_entanglement"  # Quantum-inspired entanglement vulnerability
    
    def get_description(self) -> str:
        """Get description of vulnerability pattern type."""
        descriptions = {
            VulnerabilityPattern.SPIRAL: "Spiral pattern indicating potential LCG vulnerability",
            VulnerabilityPattern.STAR: "Star pattern indicating periodic RNG vulnerability",
            VulnerabilityPattern.SYMMETRY_VIOLATION: "Symmetry violation indicating biased nonce generation",
            VulnerabilityPattern.DIAGONAL_PERIODICITY: "Diagonal periodicity indicating specific implementation vulnerability",
            VulnerabilityPattern.COLLISION_BASED: "Collision-based vulnerability indicating structural flaw",
            VulnerabilityPattern.GRADIENT_BASED: "Gradient-based vulnerability indicating key recovery potential",
            VulnerabilityPattern.STRUCTURED: "Structured topological anomaly indicating unexpected cycles",
            VulnerabilityPattern.POTENTIAL_NOISE: "Potential noise pattern indicating statistical fluctuations",
            VulnerabilityPattern.FRACTAL: "Fractal pattern indicating implementation-specific structural flaw",
            VulnerabilityPattern.WEAK_KEY: "Weak key vulnerability (gcd(d, n) > 1) indicating potential key recovery",
            VulnerabilityPattern.QUANTUM_ENTANGLEMENT: "Quantum entanglement pattern indicating unusual topological properties"
        }
        return descriptions.get(self, "Unknown vulnerability pattern")
    
    def get_criticality_weight(self) -> float:
        """Get criticality weight for this vulnerability pattern.
        
        Returns:
            Weight value (higher = more critical)
        """
        weights = {
            VulnerabilityPattern.SPIRAL: 0.7,
            VulnerabilityPattern.STAR: 0.6,
            VulnerabilityPattern.SYMMETRY_VIOLATION: 0.5,
            VulnerabilityPattern.DIAGONAL_PERIODICITY: 0.6,
            VulnerabilityPattern.COLLISION_BASED: 0.8,
            VulnerabilityPattern.GRADIENT_BASED: 0.9,
            VulnerabilityPattern.STRUCTURED: 0.7,
            VulnerabilityPattern.POTENTIAL_NOISE: 0.3,
            VulnerabilityPattern.FRACTAL: 0.75,
            VulnerabilityPattern.WEAK_KEY: 1.0,
            VulnerabilityPattern.QUANTUM_ENTANGLEMENT: 0.65
        }
        return weights.get(self, 0.5)
    
    @classmethod
    def from_vulnerability_type(cls, vuln_type: str) -> VulnerabilityPattern:
        """Map vulnerability type to pattern type.
        
        Args:
            vuln_type: Vulnerability type string
            
        Returns:
            Corresponding vulnerability pattern
        """
        mapping = {
            "spiral_pattern": cls.SPIRAL,
            "star_pattern": cls.STAR,
            "symmetry_violation": cls.SYMMETRY_VIOLATION,
            "diagonal_periodicity": cls.DIAGONAL_PERIODICITY,
            "collision_based": cls.COLLISION_BASED,
            "gradient_based": cls.GRADIENT_BASED,
            "structured_vulnerability": cls.STRUCTURED,
            "potential_noise": cls.POTENTIAL_NOISE,
            "fractal": cls.FRACTAL,
            "weak_key": cls.WEAK_KEY,
            "quantum_entanglement": cls.QUANTUM_ENTANGLEMENT
        }
        return mapping.get(vuln_type, cls.POTENTIAL_NOISE)


class VulnerabilitySeverity(Enum):
    """Severity levels for detected vulnerabilities."""
    CRITICAL = "critical"  # Immediate risk of key recovery
    HIGH = "high"  # High risk of vulnerability exploitation
    MEDIUM = "medium"  # Medium risk requiring attention
    LOW = "low"  # Low risk, potential for future issues
    INFORMATIONAL = "informational"  # Informational finding, no immediate risk
    
    def get_threshold(self) -> float:
        """Get criticality threshold for this severity level.
        
        Returns:
            Threshold value (0-1)
        """
        thresholds = {
            VulnerabilitySeverity.CRITICAL: 0.8,
            VulnerabilitySeverity.HIGH: 0.6,
            VulnerabilitySeverity.MEDIUM: 0.4,
            VulnerabilitySeverity.LOW: 0.2,
            VulnerabilitySeverity.INFORMATIONAL: 0.0
        }
        return thresholds.get(self, 0.0)
    
    @classmethod
    def from_criticality(cls, criticality: float) -> VulnerabilitySeverity:
        """Map criticality score to severity level.
        
        Args:
            criticality: Criticality score (0-1)
            
        Returns:
            Corresponding severity level
        """
        if criticality >= 0.8:
            return cls.CRITICAL
        elif criticality >= 0.6:
            return cls.HIGH
        elif criticality >= 0.4:
            return cls.MEDIUM
        elif criticality >= 0.2:
            return cls.LOW
        else:
            return cls.INFORMATIONAL


# ======================
# DATA CLASSES
# ======================

@dataclass
class Vulnerability:
    """Represents a detected vulnerability in ECDSA implementation."""
    id: str  # Unique identifier
    pattern: VulnerabilityPattern  # Type of vulnerability pattern
    criticality: float  # Criticality score (0-1, higher = more critical)
    confidence: float  # Confidence in detection (0-1, higher = more confident)
    location: Tuple[float, float]  # (u_r, u_z) location of the vulnerability
    stability: float  # Stability of the vulnerability pattern
    geometric_properties: Dict[str, Any] = field(default_factory=dict)
    description: str = ""  # Detailed description
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "id": self.id,
            "pattern": self.pattern.value,
            "criticality": self.criticality,
            "confidence": self.confidence,
            "location": self.location,
            "stability": self.stability,
            "geometric_properties": self.geometric_properties,
            "description": self.description,
            "meta": self.meta
        }
    
    @property
    def severity(self) -> VulnerabilitySeverity:
        """Get severity level based on criticality."""
        return VulnerabilitySeverity.from_criticality(self.criticality)
    
    def get_recommendation(self) -> str:
        """Get remediation recommendation based on vulnerability type."""
        recommendations = {
            VulnerabilityPattern.SPIRAL: "Replace the random number generator with a cryptographically secure implementation that does not exhibit linear congruential patterns.",
            VulnerabilityPattern.STAR: "Address the periodicity in the random number generator that is causing star pattern vulnerabilities.",
            VulnerabilityPattern.SYMMETRY_VIOLATION: "Fix the bias in nonce generation to restore diagonal symmetry in the signature space.",
            VulnerabilityPattern.DIAGONAL_PERIODICITY: "Investigate the specific implementation details causing diagonal periodicity and modify the signing process.",
            VulnerabilityPattern.COLLISION_BASED: "Review the signing process for structural flaws that are causing unusual collision patterns.",
            VulnerabilityPattern.GRADIENT_BASED: "Immediately rotate the affected key as private key recovery may be possible.",
            VulnerabilityPattern.STRUCTURED: "Investigate unexpected topological cycles in the signature space that indicate implementation flaws.",
            VulnerabilityPattern.FRACTAL: "Address the fractal structure in the signature space which indicates a deeper implementation vulnerability.",
            VulnerabilityPattern.WEAK_KEY: "Immediately rotate the affected key as it has a weak private key (gcd(d, n) > 1).",
            VulnerabilityPattern.QUANTUM_ENTANGLEMENT: "Investigate unusual topological properties that may indicate subtle implementation flaws."
        }
        return recommendations.get(self.pattern, "Review the implementation for potential cryptographic weaknesses.")


@dataclass
class VulnerabilityAssessment:
    """Results of comprehensive vulnerability assessment."""
    vulnerabilities: List[Vulnerability]
    overall_criticality: float  # Overall criticality score (0-1)
    stability_score: float  # Overall stability score (0-1)
    security_level: str  # Security level (secure, caution, vulnerable, critical)
    vulnerability_score: float  # Vulnerability score (0-1, higher = more vulnerable)
    critical_vulnerabilities: List[str]  # IDs of critical vulnerabilities
    recommendations: List[str]  # Remediation recommendations
    execution_time: float = 0.0
    analysis_timestamp: float = field(default_factory=lambda: datetime.now().timestamp())
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "vulnerabilities_count": len(self.vulnerabilities),
            "vulnerabilities": [v.to_dict() for v in self.vulnerabilities],
            "overall_criticality": self.overall_criticality,
            "stability_score": self.stability_score,
            "security_level": self.security_level,
            "vulnerability_score": self.vulnerability_score,
            "critical_vulnerabilities": self.critical_vulnerabilities,
            "recommendations": self.recommendations,
            "execution_time": self.execution_time,
            "analysis_timestamp": self.analysis_timestamp,
            "meta": self.meta
        }


@dataclass
class VulnerabilityPredictionModel:
    """Machine learning model for predicting vulnerabilities based on topological features."""
    feature_importance: Dict[str, float]  # Feature importance scores
    model_version: str  # Version of the prediction model
    training_data_size: int  # Size of training data
    accuracy: float  # Model accuracy
    execution_time: float = 0.0
    training_timestamp: float = field(default_factory=lambda: datetime.now().timestamp())
    meta: Dict[str, Any] = field(default_factory=dict)
    
    def predict(self, analysis_result: TopologicalAnalysisResult) -> Dict[str, Any]:
        """Predict vulnerability probability based on topological analysis.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary with prediction results
        """
        # Extract features from analysis result
        features = self._extract_features(analysis_result)
        
        # Calculate vulnerability probability (simulated)
        vulnerability_prob = self._calculate_vulnerability_probability(features)
        
        # Get feature importance for explanation
        feature_importance = self._get_feature_importance(features)
        
        return {
            'vulnerability_probability': vulnerability_prob,
            'is_vulnerable': vulnerability_prob > 0.5,
            'explanation': self._generate_explanation(feature_importance, analysis_result),
            'critical_features': self._identify_critical_features(feature_importance),
            'feature_scores': feature_importance
        }
    
    def _extract_features(self, analysis_result: TopologicalAnalysisResult) -> Dict[str, float]:
        """Extract features from topological analysis result.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary of feature values
        """
        features = {}
        
        # Betti number features
        features['betti0_deviation'] = abs(analysis_result.betti_numbers.beta_0 - 1.0)
        features['betti1_deviation'] = abs(analysis_result.betti_numbers.beta_1 - 2.0)
        features['betti2_deviation'] = abs(analysis_result.betti_numbers.beta_2 - 1.0)
        
        # Stability features
        features['stability_score'] = analysis_result.stability_metrics.get('score', 0.5)
        features['spiral_consistency'] = analysis_result.stability_metrics.get('spiral_consistency', 0.5)
        features['symmetry_violation'] = analysis_result.stability_metrics.get('symmetry_violation', 0.5)
        
        # Pattern features
        features['fractal_dimension'] = analysis_result.fractal_dimension
        features['spiral_score'] = analysis_result.spiral_score
        features['star_score'] = analysis_result.star_score
        features['uniformity_score'] = analysis_result.uniformity_score
        
        # Vulnerability indicators
        features['anomaly_score'] = analysis_result.anomaly_score
        features['critical_regions'] = len(analysis_result.critical_regions or [])
        
        return features
    
    def _calculate_vulnerability_probability(self, features: Dict[str, float]) -> float:
        """Calculate vulnerability probability based on features.
        
        Args:
            features: Feature values
            
        Returns:
            Vulnerability probability (0-1)
        """
        # In a real implementation, this would use a trained ML model
        # For demonstration, we'll simulate a prediction
        
        # Base score from critical features
        base_score = (
            features['betti1_deviation'] * 0.3 +
            (1.0 - features['stability_score']) * 0.2 +
            features['anomaly_score'] * 0.2 +
            features['spiral_score'] * 0.1 +
            features['star_score'] * 0.1 +
            features['symmetry_violation'] * 0.1
        )
        
        # Cap at 1.0
        return min(1.0, base_score)
    
    def _get_feature_importance(self, features: Dict[str, float]) -> Dict[str, float]:
        """Get importance of each feature for the prediction.
        
        Args:
            features: Feature values
            
        Returns:
            Dictionary of feature importance scores
        """
        # In a real implementation, this would use SHAP or similar
        # For demonstration, we'll simulate feature importance
        
        importance = {}
        for feature in features:
            # Simulate importance based on feature value
            importance[feature] = features[feature] * self.feature_importance.get(feature, 0.1)
        
        # Normalize to sum to 1.0
        total = sum(importance.values())
        if total > 0:
            for feature in importance:
                importance[feature] /= total
        
        return importance
    
    def _generate_explanation(self, 
                             feature_importance: Dict[str, float],
                             analysis_result: TopologicalAnalysisResult) -> str:
        """Generate human-readable explanation of the prediction.
        
        Args:
            feature_importance: Feature importance scores
            analysis_result: Topological analysis result
            
        Returns:
            Explanation string
        """
        # Find top 3 important features
        top_features = sorted(
            feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:3]
        
        explanations = []
        for feature, importance in top_features:
            value = self._get_feature_value(feature, analysis_result)
            
            if feature == 'betti1_deviation':
                explanations.append(
                    f"Deviation in β₁ ({value:.4f} vs expected 2.0) indicates potential topological anomalies"
                )
            elif feature == 'stability_score':
                explanations.append(
                    f"Low topological stability ({value:.4f}) suggests implementation vulnerabilities"
                )
            elif feature == 'spiral_score':
                explanations.append(
                    f"High spiral pattern score ({value:.4f}) indicates potential LCG vulnerability"
                )
            elif feature == 'star_score':
                explanations.append(
                    f"High star pattern score ({value:.4f}) indicates periodic RNG vulnerability"
                )
            elif feature == 'symmetry_violation':
                explanations.append(
                    f"Symmetry violation rate ({value:.4f}) indicates biased nonce generation"
                )
            elif feature == 'anomaly_score':
                explanations.append(
                    f"High anomaly score ({value:.4f}) indicates significant deviations from expected patterns"
                )
        
        return " | ".join(explanations) if explanations else "Vulnerability detected based on multiple topological indicators"
    
    def _get_feature_value(self, feature: str, analysis_result: TopologicalAnalysisResult) -> float:
        """Get value of a feature from analysis result.
        
        Args:
            feature: Feature name
            analysis_result: Topological analysis result
            
        Returns:
            Feature value
        """
        if feature == 'betti0_deviation':
            return abs(analysis_result.betti_numbers.beta_0 - 1.0)
        elif feature == 'betti1_deviation':
            return abs(analysis_result.betti_numbers.beta_1 - 2.0)
        elif feature == 'betti2_deviation':
            return abs(analysis_result.betti_numbers.beta_2 - 1.0)
        elif feature == 'stability_score':
            return analysis_result.stability_metrics.get('score', 0.5)
        elif feature == 'spiral_consistency':
            return analysis_result.stability_metrics.get('spiral_consistency', 0.5)
        elif feature == 'symmetry_violation':
            return analysis_result.stability_metrics.get('symmetry_violation', 0.5)
        elif feature == 'fractal_dimension':
            return analysis_result.fractal_dimension
        elif feature == 'spiral_score':
            return analysis_result.spiral_score
        elif feature == 'star_score':
            return analysis_result.star_score
        elif feature == 'uniformity_score':
            return analysis_result.uniformity_score
        elif feature == 'anomaly_score':
            return analysis_result.anomaly_score
        elif feature == 'critical_regions':
            return len(analysis_result.critical_regions or [])
        else:
            return 0.0
    
    def _identify_critical_features(self, feature_importance: Dict[str, float]) -> List[str]:
        """Identify critical features that contributed most to vulnerability.
        
        Args:
            feature_importance: Feature importance scores
            
        Returns:
            List of critical feature names
        """
        # Sort by importance
        sorted_features = sorted(
            feature_importance.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        # Return top 3 features
        return [feature for feature, _ in sorted_features[:3]]


# ======================
# VULNERABILITY MODEL CLASS
# ======================

class VulnerabilityModel:
    """TopoSphere Vulnerability Model - Comprehensive vulnerability identification and classification.
    
    This class implements the industrial-grade standards of AuditCore v3.2, providing
    mathematically rigorous identification and classification of ECDSA implementation
    vulnerabilities. The model is designed to detect multiple vulnerability types through
    precise analysis of topological structures and their deviations from expected patterns.
    
    Key features:
    - Comprehensive vulnerability classification system with precise localization
    - Quantum-inspired vulnerability scoring with adaptive parameters
    - Integration with TCON (Topological Conformance) verification
    - Fixed resource profile enforcement to prevent timing/volume analysis
    - Multiscale analysis for vulnerability detection across different scales
    
    The model is based on the mathematical principle that for secure ECDSA implementations,
    the signature space forms a topological torus with specific properties. Deviations from these
    properties in specific patterns indicate potential vulnerabilities in the implementation.
    
    Example:
        model = VulnerabilityModel(config)
        assessment = model.assess(analysis_result)
        for vuln in assessment.vulnerabilities:
            print(f"Detected {vuln.pattern.value} with criticality {vuln.criticality:.4f}")
    """
    
    def __init__(self,
                config: DynamicAnalysisConfig,
                prediction_model: Optional[VulnerabilityPredictionModel] = None):
        """Initialize the Vulnerability Model.
        
        Args:
            config: Dynamic analysis configuration
            prediction_model: Optional prediction model for vulnerability forecasting
            
        Raises:
            RuntimeError: If critical dependencies are missing
        """
        # Validate dependencies
        if not TDALIB_AVAILABLE:
            raise RuntimeError("giotto-tda library is required but not available")
        
        # Set configuration
        self.config = config
        self.curve = config.curve
        self.n = self.curve.n
        self.logger = self._setup_logger()
        
        # Initialize components
        self.prediction_model = prediction_model or self._create_default_prediction_model()
        
        # Initialize state
        self.last_assessment: Dict[str, VulnerabilityAssessment] = {}
        self.assessment_cache: Dict[str, VulnerabilityAssessment] = {}
        
        self.logger.info("Initialized VulnerabilityModel for comprehensive vulnerability identification")
    
    def _setup_logger(self):
        """Set up logger for the model."""
        logger = logging.getLogger("TopoSphere.VulnerabilityModel")
        logger.setLevel(self.config.log_level)
        
        # Add console handler if none exists
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _create_default_prediction_model(self) -> VulnerabilityPredictionModel:
        """Create a default prediction model with standard feature importance.
        
        Returns:
            VulnerabilityPredictionModel object
        """
        return VulnerabilityPredictionModel(
            feature_importance={
                'betti1_deviation': 0.3,
                'stability_score': 0.2,
                'anomaly_score': 0.2,
                'spiral_score': 0.1,
                'star_score': 0.1,
                'symmetry_violation': 0.1
            },
            model_version="1.0.0",
            training_data_size=1000,
            accuracy=0.85
        )
    
    def assess(self,
              analysis_result: TopologicalAnalysisResult,
              force_reassessment: bool = False) -> VulnerabilityAssessment:
        """Assess vulnerabilities based on topological analysis.
        
        Args:
            analysis_result: Topological analysis result to assess
            force_reassessment: Whether to force reassessment even if recent
            
        Returns:
            VulnerabilityAssessment object with assessment results
            
        Raises:
            RuntimeError: If assessment fails
            ValueError: If analysis result is invalid
        """
        start_time = time.time()
        self.logger.info("Performing vulnerability assessment...")
        
        # Generate cache key
        cache_key = f"{analysis_result.public_key[:16]}_{analysis_result.curve}"
        
        # Check cache
        if not force_reassessment and cache_key in self.last_assessment:
            last_assess = self.last_assessment[cache_key].analysis_timestamp
            if time.time() - last_assess < 3600:  # 1 hour
                self.logger.info(
                    f"Using cached vulnerability assessment for key {analysis_result.public_key[:16]}..."
                )
                return self.last_assessment[cache_key]
        
        try:
            # Identify vulnerabilities
            vulnerabilities = self._identify_vulnerabilities(analysis_result)
            
            # Calculate overall criticality
            overall_criticality = self._calculate_overall_criticality(vulnerabilities)
            
            # Calculate stability score
            stability_score = analysis_result.stability_metrics.get("score", 0.5)
            
            # Calculate vulnerability score
            vulnerability_score = self._calculate_vulnerability_score(
                analysis_result,
                overall_criticality,
                stability_score
            )
            
            # Determine security level
            security_level = self._get_security_level(vulnerability_score)
            
            # Get critical vulnerabilities
            critical_vulnerabilities = [
                vuln.id for vuln in vulnerabilities 
                if vuln.severity == VulnerabilitySeverity.CRITICAL
            ]
            
            # Get recommendations
            recommendations = self._get_recommendations(vulnerabilities)
            
            # Create assessment result
            assessment = VulnerabilityAssessment(
                vulnerabilities=vulnerabilities,
                overall_criticality=overall_criticality,
                stability_score=stability_score,
                security_level=security_level,
                vulnerability_score=vulnerability_score,
                critical_vulnerabilities=critical_vulnerabilities,
                recommendations=recommendations,
                execution_time=time.time() - start_time,
                meta={
                    "curve": analysis_result.curve,
                    "anomaly_score": analysis_result.anomaly_score,
                    "vulnerability_prediction": self.prediction_model.predict(analysis_result)
                }
            )
            
            # Cache results
            self.last_assessment[cache_key] = assessment
            self.assessment_cache[cache_key] = assessment
            
            self.logger.info(
                f"Vulnerability assessment completed in {time.time() - start_time:.4f}s. "
                f"Vulnerability score: {vulnerability_score:.4f}, Security level: {security_level}"
            )
            
            return assessment
            
        except Exception as e:
            self.logger.error(f"Vulnerability assessment failed: {str(e)}")
            raise RuntimeError(f"Failed to assess vulnerabilities: {str(e)}") from e
    
    def _identify_vulnerabilities(self,
                                 analysis_result: TopologicalAnalysisResult) -> List[Vulnerability]:
        """Identify vulnerabilities based on topological analysis.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            List of identified vulnerabilities
        """
        vulnerabilities = []
        public_key = analysis_result.public_key
        n = self.n
        
        # 1. Check for structured vulnerability (additional cycles)
        beta1_deviation = abs(analysis_result.betti_numbers.beta_1 - 2.0)
        if beta1_deviation > self.config.betti_tolerance * 2.0:
            criticality = min(1.0, beta1_deviation * 1.5)
            confidence = max(0.5, 1.0 - beta1_deviation)
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 2, n / 2)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"STRUCTURED-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.STRUCTURED,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - beta1_deviation,
                description=f"Additional topological cycles detected (beta_1 deviation: {beta1_deviation:.4f})",
                meta={
                    "beta1_deviation": beta1_deviation,
                    "expected_beta1": 2.0,
                    "actual_beta1": analysis_result.betti_numbers.beta_1
                }
            ))
        
        # 2. Check for spiral pattern vulnerability
        spiral_score = analysis_result.spiral_score
        if spiral_score < 0.7:  # Threshold for spiral pattern vulnerability
            criticality = min(1.0, (0.7 - spiral_score) * 1.5)
            confidence = min(0.95, spiral_score * 1.2)
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 4, n / 4)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"SPIRAL-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.SPIRAL,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=spiral_score,
                description="Spiral pattern inconsistency indicating potential LCG vulnerability",
                meta={
                    "spiral_score": spiral_score,
                    "threshold": 0.7
                }
            ))
        
        # 3. Check for star pattern vulnerability
        star_score = analysis_result.star_score
        if star_score > 0.6:  # Threshold for star pattern vulnerability
            criticality = min(1.0, (star_score - 0.6) * 1.5)
            confidence = min(0.95, 1.0 - star_score)
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 2, n / 4)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"STAR-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.STAR,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - star_score,
                description="Star pattern indicating periodic RNG vulnerability",
                meta={
                    "star_score": star_score,
                    "threshold": 0.6
                }
            ))
        
        # 4. Check for symmetry violation
        symmetry_violation = analysis_result.stability_metrics.get("symmetry_violation", 0.0)
        if symmetry_violation > 0.01:  # Threshold for symmetry violation
            criticality = min(1.0, symmetry_violation * 1.5)
            confidence = min(0.95, 1.0 - symmetry_violation)
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 4, n / 2)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"SYMMETRY-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.SYMMETRY_VIOLATION,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - symmetry_violation,
                description="Diagonal symmetry violation indicating biased nonce generation",
                meta={
                    "symmetry_violation": symmetry_violation,
                    "threshold": 0.01
                }
            ))
        
        # 5. Check for diagonal periodicity
        diagonal_periodicity = analysis_result.stability_metrics.get("diagonal_periodicity", 0.0)
        if diagonal_periodicity > 0.7:  # Threshold for diagonal periodicity
            criticality = min(1.0, (diagonal_periodicity - 0.7) * 1.5)
            confidence = min(0.95, diagonal_periodicity * 1.2)
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 2, n / 2)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"DIAGONAL-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.DIAGONAL_PERIODICITY,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=diagonal_periodicity,
                description="Diagonal periodicity indicating specific implementation vulnerability",
                meta={
                    "diagonal_periodicity": diagonal_periodicity,
                    "threshold": 0.7
                }
            ))
        
        # 6. Check for potential noise issues
        if (not vulnerabilities and 
            analysis_result.anomaly_score > 0.3 and 
            analysis_result.anomaly_score < 0.5):
            criticality = analysis_result.anomaly_score * 0.8
            confidence = 1.0 - analysis_result.anomaly_score
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 3, n / 3)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"NOISE-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.POTENTIAL_NOISE,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - analysis_result.anomaly_score,
                description="Potential noise in topological structure requiring further analysis",
                meta={
                    "anomaly_score": analysis_result.anomaly_score,
                    "range": "0.3-0.5"
                }
            ))
        
        # 7. Check for weak key vulnerability
        if analysis_result.weak_key_gcd and analysis_result.weak_key_gcd > 1:
            criticality = min(1.0, analysis_result.weak_key_gcd / n)
            confidence = 0.9
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 5, n / 5)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"WEAKKEY-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.WEAK_KEY,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - criticality,
                description=f"Weak key detected (gcd(d, n) = {analysis_result.weak_key_gcd})",
                meta={
                    "gcd": analysis_result.weak_key_gcd,
                    "n": n
                }
            ))
        
        # 8. Check for quantum entanglement patterns
        if analysis_result.quantum_entanglement_score > 0.8:
            criticality = min(1.0, (analysis_result.quantum_entanglement_score - 0.8) * 2.0)
            confidence = analysis_result.quantum_entanglement_score
            
            # Estimate location from critical regions
            location = self._estimate_vulnerability_location(
                analysis_result.critical_regions,
                (n / 6, n / 6)
            )
            
            vulnerabilities.append(Vulnerability(
                id=f"QUANTUM-{secrets.token_hex(4)}",
                pattern=VulnerabilityPattern.QUANTUM_ENTANGLEMENT,
                criticality=criticality,
                confidence=confidence,
                location=location,
                stability=1.0 - criticality,
                description="Quantum entanglement pattern indicating unusual topological properties",
                meta={
                    "entanglement_score": analysis_result.quantum_entanglement_score,
                    "threshold": 0.8
                }
            ))
        
        return vulnerabilities
    
    def _estimate_vulnerability_location(self,
                                       critical_regions: List[Dict[str, Any]],
                                       default_location: Tuple[float, float]) -> Tuple[float, float]:
        """Estimate location of a vulnerability based on critical regions.
        
        Args:
            critical_regions: Critical regions with topological anomalies
            default_location: Default location if no critical regions
            
        Returns:
            Estimated location (u_r, u_z)
        """
        if not critical_regions:
            return default_location
        
        # Find the most critical region
        most_critical = max(
            critical_regions,
            key=lambda r: r.get("criticality", 0.0)
        )
        
        # Return center of the region
        u_r_min, u_r_max = most_critical["u_r_range"]
        u_z_min, u_z_max = most_critical["u_z_range"]
        return (
            (u_r_min + u_r_max) / 2.0,
            (u_z_min + u_z_max) / 2.0
        )
    
    def _calculate_overall_criticality(self, vulnerabilities: List[Vulnerability]) -> float:
        """Calculate overall criticality score from vulnerabilities.
        
        Args:
            vulnerabilities: List of identified vulnerabilities
            
        Returns:
            Overall criticality score (0-1)
        """
        if not vulnerabilities:
            return 0.0
        
        # Weighted average of criticalities (higher criticality has more impact)
        weighted_sum = 0.0
        total_weight = 0.0
        
        for vuln in vulnerabilities:
            weight = vuln.pattern.get_criticality_weight()
            weighted_sum += vuln.criticality * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _calculate_vulnerability_score(self,
                                      analysis_result: TopologicalAnalysisResult,
                                      overall_criticality: float,
                                      stability_score: float) -> float:
        """Calculate vulnerability score from analysis results.
        
        Args:
            analysis_result: Topological analysis result
            overall_criticality: Overall criticality score
            stability_score: Stability score
            
        Returns:
            Vulnerability score (0-1, higher = more vulnerable)
        """
        # Base score from criticality
        base_score = overall_criticality
        
        # Add penalties for specific issues
        penalties = []
        
        # Betti number deviations
        beta1_deviation = abs(analysis_result.betti_numbers.beta_1 - 2.0)
        if beta1_deviation > 0.3:
            penalties.append(beta1_deviation * 0.5)
        
        # Symmetry violation
        symmetry_violation = analysis_result.stability_metrics.get("symmetry_violation", 0.0)
        if symmetry_violation > 0.01:
            penalties.append(symmetry_violation * 0.4)
        
        # Anomaly score
        if analysis_result.anomaly_score > 0.2:
            penalties.append(analysis_result.anomaly_score * 0.3)
        
        # Stability penalty
        stability_penalty = (1.0 - stability_score) * 0.2
        
        # Calculate final score
        vulnerability_score = base_score + sum(penalties) + stability_penalty
        return min(1.0, vulnerability_score)
    
    def _get_security_level(self, vulnerability_score: float) -> str:
        """Get security level based on vulnerability score.
        
        Args:
            vulnerability_score: Vulnerability score (0-1)
            
        Returns:
            Security level as string
        """
        if vulnerability_score < self.config.vulnerability_threshold:
            return "secure"
        elif vulnerability_score < 0.4:
            return "caution"
        elif vulnerability_score < self.config.critical_vulnerability_threshold:
            return "vulnerable"
        else:
            return "critical"
    
    def _get_recommendations(self, vulnerabilities: List[Vulnerability]) -> List[str]:
        """Get remediation recommendations for identified vulnerabilities.
        
        Args:
            vulnerabilities: List of identified vulnerabilities
            
        Returns:
            List of recommendations
        """
        recommendations = set()
        
        for vuln in vulnerabilities:
            # Only include critical and high severity recommendations
            if vuln.severity in [VulnerabilitySeverity.CRITICAL, VulnerabilitySeverity.HIGH]:
                recommendations.add(vuln.get_recommendation())
        
        return list(recommendations)
    
    def predict_vulnerability(self,
                             analysis_result: TopologicalAnalysisResult) -> Dict[str, Any]:
        """Predict vulnerability probability using machine learning model.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary with prediction results
        """
        return self.prediction_model.predict(analysis_result)
    
    def get_vulnerability_report(self,
                                analysis_result: TopologicalAnalysisResult,
                                assessment: Optional[VulnerabilityAssessment] = None) -> str:
        """Get human-readable vulnerability report.
        
        Args:
            analysis_result: Topological analysis result
            assessment: Optional vulnerability assessment
            
        Returns:
            Vulnerability report as string
        """
        if assessment is None:
            assessment = self.assess(analysis_result)
        
        lines = [
            "=" * 80,
            "VULNERABILITY ASSESSMENT REPORT",
            "=" * 80,
            f"Assessment Timestamp: {datetime.fromtimestamp(assessment.analysis_timestamp).strftime('%Y-%m-%d %H:%M:%S')}",
            f"Public Key: {analysis_result.public_key[:50]}{'...' if len(analysis_result.public_key) > 50 else ''}",
            f"Curve: {analysis_result.curve}",
            "",
            "SECURITY OVERVIEW:",
            f"Vulnerability Score: {assessment.vulnerability_score:.4f}",
            f"Security Level: {assessment.security_level.upper()}",
            f"Overall Criticality: {assessment.overall_criticality:.4f}",
            f"Stability Score: {assessment.stability_score:.4f}",
            "",
            "DETECTED VULNERABILITIES:"
        ]
        
        if not assessment.vulnerabilities:
            lines.append("  None detected")
        else:
            for i, vuln in enumerate(assessment.vulnerabilities[:5], 1):  # Show up to 5 vulnerabilities
                lines.append(f"  {i}. [{vuln.pattern.value.upper()}] {vuln.description}")
                lines.append(
                    f"     Criticality: {vuln.criticality:.4f} | Confidence: {vuln.confidence:.4f} | "
                    f"Severity: {vuln.severity.value.upper()}"
                )
                lines.append(
                    f"     Location: ({vuln.location[0]:.2f}, {vuln.location[1]:.2f})"
                )
                if vuln.geometric_properties:
                    props = ", ".join(
                        f"{k}={v:.4f}" for k, v in vuln.geometric_properties.items()
                    )
                    lines.append(f"     Properties: {props}")
            
            if len(assessment.vulnerabilities) > 5:
                lines.append(f"  - And {len(assessment.vulnerabilities) - 5} more vulnerabilities")
        
        # Add critical vulnerabilities section
        if assessment.critical_vulnerabilities:
            lines.extend([
                "",
                "CRITICAL VULNERABILITIES REQUIRING IMMEDIATE ACTION:",
                f"  - {len(assessment.critical_vulnerabilities)} critical vulnerabilities detected"
            ])
        
        # Add recommendations
        if assessment.recommendations:
            lines.extend([
                "",
                "RECOMMENDATIONS:",
                "  The following actions are recommended to address critical vulnerabilities:"
            ])
            for i, rec in enumerate(assessment.recommendations, 1):
                lines.append(f"  {i}. {rec}")
        
        # Add prediction information
        prediction = assessment.meta.get("vulnerability_prediction", {})
        if prediction:
            lines.extend([
                "",
                "VULNERABILITY PREDICTION:",
                f"  Probability: {prediction.get('vulnerability_probability', 0):.4f}",
                f"  Is Vulnerable: {'YES' if prediction.get('is_vulnerable', False) else 'NO'}"
            ])
            if prediction.get('explanation'):
                lines.append(f"  Explanation: {prediction['explanation']}")
        
        lines.extend([
            "",
            "=" * 80,
            "VULNERABILITY ASSESSMENT FOOTER",
            "=" * 80,
            "This report was generated by TopoSphere Vulnerability Model,",
            "a component of the Dynamic Analysis system for detecting ECDSA vulnerabilities.",
            "A 'secure' result does not guarantee the absence of all possible vulnerabilities.",
            "Additional security testing is recommended for critical systems.",
            "=" * 80
        ])
        
        return "\n".join(lines)
    
    def get_quantum_security_metrics(self,
                                    analysis_result: TopologicalAnalysisResult) -> Dict[str, Any]:
        """Get quantum-inspired security metrics.
        
        Args:
            analysis_result: Topological analysis result
            
        Returns:
            Dictionary with quantum security metrics
        """
        # Calculate quantum-inspired metrics
        entanglement_entropy = min(1.0, analysis_result.stability_metrics.get("score", 0.5) * 1.2)
        quantum_confidence = 1.0 - analysis_result.anomaly_score
        quantum_vulnerability_score = 1.0 - quantum_confidence
        
        return {
            "entanglement_entropy": entanglement_entropy,
            "quantum_confidence": quantum_confidence,
            "quantum_vulnerability_score": quantum_vulnerability_score,
            "quantum_entanglement_score": analysis_result.quantum_entanglement_score,
            "security_level": self._get_security_level(quantum_vulnerability_score)
        }
    
    def get_vulnerability_statistics(self,
                                    assessments: List[VulnerabilityAssessment]) -> Dict[str, Any]:
        """Get statistics about vulnerabilities across multiple assessments.
        
        Args:
            assessments: List of vulnerability assessments
            
        Returns:
            Dictionary with vulnerability statistics
        """
        if not assessments:
            return {
                "total_assessments": 0,
                "vulnerability_distribution": {},
                "criticality_distribution": {},
                "security_level_distribution": {}
            }
        
        # Count vulnerability types
        vulnerability_counts = {}
        for assessment in assessments:
            for vuln in assessment.vulnerabilities:
                vuln_type = vuln.pattern.value
                vulnerability_counts[vuln_type] = vulnerability_counts.get(vuln_type, 0) + 1
        
        # Calculate criticality distribution
        criticality_bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
        criticality_counts = [0] * (len(criticality_bins) - 1)
        
        for assessment in assessments:
            bin_idx = min(
                len(criticality_bins) - 2,
                int(assessment.overall_criticality / 0.2)
            )
            criticality_counts[bin_idx] += 1
        
        # Calculate security level distribution
        security_levels = ["secure", "caution", "vulnerable", "critical"]
        security_counts = {level: 0 for level in security_levels}
        
        for assessment in assessments:
            security_counts[assessment.security_level] += 1
        
        return {
            "total_assessments": len(assessments),
            "vulnerability_distribution": {
                vuln_type: count / len(assessments) 
                for vuln_type, count in vulnerability_counts.items()
            },
            "criticality_distribution": {
                f"{criticality_bins[i]:.1f}-{criticality_bins[i+1]:.1f}": 
                count / len(assessments)
                for i, count in enumerate(criticality_counts)
            },
            "security_level_distribution": {
                level: count / len(assessments)
                for level, count in security_counts.items()
            }
        }
